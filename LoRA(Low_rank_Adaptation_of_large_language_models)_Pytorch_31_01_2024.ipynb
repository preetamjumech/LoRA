{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4gUv2VgxBmXTsu128hKsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/preetamjumech/LoRA/blob/main/LoRA(Low_rank_Adaptation_of_large_language_models)_Pytorch_31_01_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD(Singluar Value Decomposition)"
      ],
      "metadata": {
        "id": "dWRXvsYEMmv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "_ = torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "JgFyUS6MMtuq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMlL_E01M7PZ",
        "outputId": "0abbe1a3-c3b6-4e37-b92b-de16982b98b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(2,4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNDkKEICQiUC",
        "outputId": "b9934f33-cdc1-4c96-b28c-9924ef8f4afe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.5410, -0.2934, -2.1788,  0.5684],\n",
              "        [-1.0845, -1.3986,  0.4033,  0.8380]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a rank-deficient matrix w\n",
        "d, k = 10,10\n",
        "w_rank = 2\n",
        "w = torch.randn(d,w_rank) @ torch.randn(w_rank,k)\n",
        "print(w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6tNN3UmNg7E",
        "outputId": "4af930e7-d718-4b7d-a28c-585a4dd8fcb6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.1643,  0.6744, -1.2905,  1.8246,  0.6706, -2.8475, -1.0359, -3.5343,\n",
            "          0.4691, -0.6535],\n",
            "        [-0.9974, -0.2896,  0.5678, -0.8275, -0.4137,  1.1944,  0.3716,  1.4960,\n",
            "         -0.1449,  0.3268],\n",
            "        [-0.2241, -0.1818,  0.2757, -0.2593,  0.4838,  0.9175,  0.6661,  1.0675,\n",
            "         -0.4251, -0.0676],\n",
            "        [-0.6295, -0.2886,  0.4927, -0.5888,  0.2616,  1.3423,  0.7626,  1.6071,\n",
            "         -0.4473,  0.0784],\n",
            "        [ 0.8609,  0.1478, -0.3605,  0.6502,  0.8616, -0.4631,  0.1888, -0.6515,\n",
            "         -0.2182, -0.4055],\n",
            "        [-0.4132, -0.2426,  0.3908, -0.4199,  0.4340,  1.1764,  0.7656,  1.3876,\n",
            "         -0.4721, -0.0127],\n",
            "        [-2.6355, -0.4010,  1.0383, -1.9579, -2.8924,  1.1311, -0.8353,  1.6714,\n",
            "          0.8415,  1.3035],\n",
            "        [ 2.0717,  0.7106, -1.3179,  1.7875,  0.3205, -3.0875, -1.3163, -3.7907,\n",
            "          0.6678, -0.5470],\n",
            "        [-0.3695, -0.4786,  0.6814, -0.5398,  1.6806,  2.5066,  1.9901,  2.8798,\n",
            "         -1.3018, -0.3275],\n",
            "        [ 0.2817,  0.4706, -0.6536,  0.4779, -1.8035, -2.4987, -2.0447, -2.8577,\n",
            "          1.3479,  0.3774]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the rank of matrix w\n",
        "\n",
        "w_r = np.linalg.matrix_rank(w)\n",
        "print(w_r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpZOG3j1Nquc",
        "outputId": "888e24ea-24da-4532-ab5a-6f8e5f88e6d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dot Product\n",
        "a = np.array([[1,2],[3,4]])\n",
        "b = np.array([[5,6],[7,8]])\n",
        "a*b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVUXx7ndR2tK",
        "outputId": "fe5433fd-e718-4439-e075-cecfa7a5686a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5, 12],\n",
              "       [21, 32]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#multiplication\n",
        "a@b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VWG_Vf6TipV",
        "outputId": "52dd6421-dbe6-4936-c96b-2e72eb36bcc6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[19, 22],\n",
              "       [43, 50]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "u,s,v = torch.svd(w)\n",
        "print(u)\n",
        "print(s)\n",
        "print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yibs8GqCTzHG",
        "outputId": "f5ad0b05-7f44-437e-ff90-66dd597e48fe"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4883, -0.1729,  0.8217,  0.1592, -0.0742, -0.1205,  0.0552,  0.0794,\n",
            "         -0.0088,  0.0418],\n",
            "        [ 0.2083,  0.1053,  0.0057,  0.6309, -0.1819,  0.1568, -0.4695,  0.4773,\n",
            "         -0.1032,  0.1755],\n",
            "        [ 0.1390, -0.1174,  0.1418,  0.1548, -0.0640,  0.2301, -0.2408, -0.2838,\n",
            "         -0.0824, -0.8488],\n",
            "        [ 0.2151, -0.0614,  0.0934,  0.2008,  0.1002,  0.5313,  0.7157,  0.2749,\n",
            "          0.1234, -0.0745],\n",
            "        [-0.0991, -0.2143, -0.0461, -0.0791, -0.1340,  0.2105, -0.2517,  0.0076,\n",
            "          0.8992,  0.0400],\n",
            "        [ 0.1831, -0.1045, -0.0304, -0.1534, -0.1577, -0.5900,  0.1461,  0.6002,\n",
            "          0.1481, -0.3932],\n",
            "        [ 0.2627,  0.7182,  0.3565, -0.3283, -0.3943,  0.1187, -0.0115, -0.0250,\n",
            "          0.0996,  0.0072],\n",
            "        [-0.5188, -0.0868, -0.3030, -0.2456, -0.5573,  0.3401,  0.0227,  0.2497,\n",
            "         -0.2692, -0.0979],\n",
            "        [ 0.3704, -0.4096,  0.0310,  0.1559, -0.6433, -0.1708,  0.1959, -0.3677,\n",
            "         -0.0494,  0.2253],\n",
            "        [-0.3659,  0.4400, -0.2703,  0.5386, -0.1567, -0.2725,  0.2874, -0.2264,\n",
            "          0.2278, -0.1577]])\n",
            "tensor([1.1556e+01, 5.9636e+00, 4.1689e-07, 2.9888e-07, 2.0551e-07, 1.0820e-07,\n",
            "        4.9863e-08, 2.8623e-08, 2.4899e-08, 1.5164e-08])\n",
            "tensor([[-0.3115, -0.3945, -0.0056,  0.3351,  0.0093,  0.5717,  0.2076, -0.4465,\n",
            "          0.1504,  0.2072],\n",
            "        [-0.1176, -0.0102,  0.5142, -0.5797, -0.3499,  0.0748, -0.3128, -0.3278,\n",
            "          0.2287,  0.0054],\n",
            "        [ 0.2118,  0.0923, -0.0559,  0.0642, -0.5319,  0.1362, -0.0813, -0.0058,\n",
            "         -0.5889,  0.5334],\n",
            "        [-0.2755, -0.2618,  0.1874, -0.1710,  0.0323,  0.4130, -0.0722,  0.7793,\n",
            "         -0.0763,  0.0349],\n",
            "        [ 0.0052, -0.6790,  0.0461, -0.0699,  0.2799, -0.4947, -0.2261, -0.0507,\n",
            "         -0.1094,  0.3784],\n",
            "        [ 0.5243, -0.1075,  0.6558,  0.3041, -0.0615, -0.0487,  0.3652,  0.1357,\n",
            "          0.1761,  0.0452],\n",
            "        [ 0.2518, -0.3735, -0.4716, -0.3220, -0.4543, -0.0385,  0.3636,  0.1150,\n",
            "          0.3379, -0.0596],\n",
            "        [ 0.6376, -0.0617, -0.1362, -0.2784,  0.4329,  0.4784, -0.2380, -0.1228,\n",
            "         -0.0692, -0.0278],\n",
            "        [-0.1367,  0.2934,  0.0433, -0.4429,  0.3428, -0.0264,  0.5780, -0.0265,\n",
            "         -0.0033,  0.4942],\n",
            "        [ 0.0692,  0.2553, -0.1494,  0.2089, -0.0096,  0.0143, -0.3738,  0.1893,\n",
            "          0.6407,  0.5269]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For rank-r factorization, keeping only the first r singular values\n",
        "u_r = u[:,:w_r]\n",
        "print(u_r)\n",
        "s_r = torch.diag(s[:w_r])\n",
        "print(s_r)\n",
        "v_r = v[:,:w_r].T\n",
        "print(v_r)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcP7AemUaMDi",
        "outputId": "73c39979-e2d6-491a-ca0f-6b4da5996653"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4883, -0.1729],\n",
            "        [ 0.2083,  0.1053],\n",
            "        [ 0.1390, -0.1174],\n",
            "        [ 0.2151, -0.0614],\n",
            "        [-0.0991, -0.2143],\n",
            "        [ 0.1831, -0.1045],\n",
            "        [ 0.2627,  0.7182],\n",
            "        [-0.5188, -0.0868],\n",
            "        [ 0.3704, -0.4096],\n",
            "        [-0.3659,  0.4400]])\n",
            "tensor([[11.5557,  0.0000],\n",
            "        [ 0.0000,  5.9636]])\n",
            "tensor([[-0.3115, -0.1176,  0.2118, -0.2755,  0.0052,  0.5243,  0.2518,  0.6376,\n",
            "         -0.1367,  0.0692],\n",
            "        [-0.3945, -0.0102,  0.0923, -0.2618, -0.6790, -0.1075, -0.3735, -0.0617,\n",
            "          0.2934,  0.2553]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = u_r@s_r\n",
        "A = v_r\n",
        "print(B.shape)\n",
        "print(A.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NwA_IMvaMHD",
        "outputId": "582c62f0-9e64-4698-9213-22e850885c76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 2])\n",
            "torch.Size([2, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bias = torch.randn(d)\n",
        "bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahA_noLhiK5V",
        "outputId": "b4e070a7-1659-48b3-b76d-dae2a5a06b9a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.1318,  1.0198, -0.4468,  0.4520, -0.9759,  0.7112, -0.7582, -0.6436,\n",
              "        -0.6462, -0.1591])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(d)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7EikdMViK8-",
        "outputId": "ca991b14-f597-46f3-bdbf-3bb229bac03b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.7787,  0.8477,  0.2459, -0.1312, -0.1785, -0.5959,  0.2739,  0.5679,\n",
              "        -0.6731, -1.2095])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.shape)\n",
        "print(x.shape)\n",
        "print(bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA5YcWIQjHGy",
        "outputId": "0cf204cf-2bb2-4b59-bea4-d4bb28e0765c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 10])\n",
            "torch.Size([10])\n",
            "torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = w@x + bias\n",
        "print(y.shape)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RODAfB6iLAS",
        "outputId": "1f0e826f-edf2-4aae-c22f-a76a931b05b8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "tensor([-3.9420,  2.8123,  0.4229,  2.0068, -2.1147,  1.9442,  2.5216, -4.8034,\n",
            "         1.5030, -2.2204])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_prime = (B@A)@x + bias\n",
        "print(y_prime.shape)\n",
        "print(y_prime)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF-7FseGjVqt",
        "outputId": "e4a7d094-1b2d-449e-da37-dfad14ac8cbd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10])\n",
            "tensor([-3.9420,  2.8123,  0.4229,  2.0068, -2.1147,  1.9442,  2.5216, -4.8034,\n",
            "         1.5030, -2.2204])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no of parameters\n",
        "print(type(w))\n",
        "print(w.nelement())\n",
        "print(B.nelement() + A.nelement())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtS-JXKDjzxm",
        "outputId": "ffc3a3a2-f51c-44fe-dedb-e7595aabc226"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "100\n",
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA implementation using PyTorch"
      ],
      "metadata": {
        "id": "bqj8sstxNuve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YV6LxxhaN2cK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "09NNpBhYORGz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,))])\n",
        "mnist_trainset = datasets.MNIST(root = './data',train = True, download = True, transform = transform)\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size = 10, shuffle = True)\n",
        "\n",
        "mnist_testset = datasets.MNIST(root = './data',train = False, download = True, transform = transform)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size = 10, shuffle = True)"
      ],
      "metadata": {
        "id": "pbwvB4WPPEG3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RZtHAkQTPEKj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NormalNet(nn.Module):\n",
        "  def __init__(self,hidden_size_1 = 1000, hidden_size_2 = 2000):\n",
        "    super(NormalNet,self).__init__()\n",
        "    self.linear1 = nn.Linear(28*28,hidden_size_1)\n",
        "    self.linear2 = nn.Linear(hidden_size_1,hidden_size_2)\n",
        "    self.linear3 = nn.Linear(hidden_size_2,10)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, img):\n",
        "    x = img.view(-1,28*28)\n",
        "    x = self.relu(self.linear1(x))\n",
        "    x = self.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    return x\n",
        "\n",
        "net = NormalNet().to(device)"
      ],
      "metadata": {
        "id": "3I1_3qDFPEOb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, net, epochs=5, total_iterations_limit=None):\n",
        "    cross_el = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "    total_iterations = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "\n",
        "        loss_sum = 0\n",
        "        num_iterations = 0\n",
        "\n",
        "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
        "        if total_iterations_limit is not None:\n",
        "            data_iterator.total = total_iterations_limit\n",
        "        for data in data_iterator:\n",
        "            num_iterations += 1\n",
        "            total_iterations += 1\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = net(x.view(-1, 28*28))\n",
        "            loss = cross_el(output, y)\n",
        "            loss_sum += loss.item()\n",
        "            avg_loss = loss_sum / num_iterations\n",
        "            data_iterator.set_postfix(loss=avg_loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
        "                return\n",
        "\n",
        "train(train_loader, net, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPRvDrvIPERk",
        "outputId": "0381566c-7951-49a6-ff76-a0cdbab8ceab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 6000/6000 [08:34<00:00, 11.67it/s, loss=0.236]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keeping a copy of the original weights (cloning them) so later we can prove that a fine-tuning with LoRA doesn't alter the original weights\n",
        "original_weights = {}\n",
        "for name, param in net.named_parameters():\n",
        "    original_weights[name] = param.clone().detach()"
      ],
      "metadata": {
        "id": "3d5qTPxwPEUb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(original_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfc5D8i4cVjn",
        "outputId": "73e1f4bf-c171-4f4e-a24d-e03af585bf0b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    wrong_counts = [0 for i in range(10)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(test_loader, desc='Testing'):\n",
        "            x, y = data\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            output = net(x.view(-1, 784))\n",
        "            for idx, i in enumerate(output):\n",
        "                if torch.argmax(i) == y[idx]:\n",
        "                    correct +=1\n",
        "                else:\n",
        "                    wrong_counts[y[idx]] +=1\n",
        "                total +=1\n",
        "    print(f'Accuracy: {round(correct/total, 3)}')\n",
        "    for i in range(len(wrong_counts)):\n",
        "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4iAqbdwPEXK",
        "outputId": "532326fa-2af6-490a-a1c8-35ab897318c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:07<00:00, 136.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.954\n",
            "wrong counts for the digit 0: 31\n",
            "wrong counts for the digit 1: 17\n",
            "wrong counts for the digit 2: 46\n",
            "wrong counts for the digit 3: 74\n",
            "wrong counts for the digit 4: 29\n",
            "wrong counts for the digit 5: 7\n",
            "wrong counts for the digit 6: 36\n",
            "wrong counts for the digit 7: 80\n",
            "wrong counts for the digit 8: 25\n",
            "wrong counts for the digit 9: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the size of the weights matrices of the network\n",
        "# Save the count of the total number of parameters\n",
        "total_parameters_original = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
        "print(f'Total number of parameters: {total_parameters_original:,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLdaEXFNPEZ6",
        "outputId": "ef15c773-80a1-448a-d8ca-92045f0bd5f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n",
            "Total number of parameters: 2,807,010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LoRAParametrization(nn.Module):\n",
        "    def __init__(self, features_in, features_out, rank=1, alpha=1, device='cpu'):\n",
        "        super().__init__()\n",
        "        # Section 4.1 of the paper:\n",
        "        #   We use a random Gaussian initialization for A and zero for B, so ∆W = BA is zero at the beginning of training\n",
        "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
        "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
        "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
        "\n",
        "        # Section 4.1 of the paper:\n",
        "        #   We then scale ∆Wx by α/r , where α is a constant in r.\n",
        "        #   When optimizing with Adam, tuning α is roughly the same as tuning the learning rate if we scale the initialization appropriately.\n",
        "        #   As a result, we simply set α to the first r we try and do not tune it.\n",
        "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
        "        self.scale = alpha / rank\n",
        "        self.enabled = True\n",
        "\n",
        "    def forward(self, original_weights):\n",
        "        if self.enabled:\n",
        "            # Return W + (B*A)*scale\n",
        "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
        "        else:\n",
        "            return original_weights"
      ],
      "metadata": {
        "id": "6nZXL_TJO--7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
        "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
        "\n",
        "    # From section 4.2 of the paper:\n",
        "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
        "    #   [...]\n",
        "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
        "\n",
        "    features_in, features_out = layer.weight.shape\n",
        "    return LoRAParametrization(\n",
        "        features_in, features_out, rank=rank, alpha=lora_alpha, device=device\n",
        "    )\n",
        "\n",
        "parametrize.register_parametrization(\n",
        "    net.linear1, \"weight\", linear_layer_parameterization(net.linear1, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear2, \"weight\", linear_layer_parameterization(net.linear2, device)\n",
        ")\n",
        "parametrize.register_parametrization(\n",
        "    net.linear3, \"weight\", linear_layer_parameterization(net.linear3, device)\n",
        ")\n",
        "\n",
        "\n",
        "def enable_disable_lora(enabled=True):\n",
        "    for layer in [net.linear1, net.linear2, net.linear3]:\n",
        "        layer.parametrizations[\"weight\"][0].enabled = enabled"
      ],
      "metadata": {
        "id": "kxP87qceZ7cL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_parameters_lora = 0\n",
        "total_parameters_non_lora = 0\n",
        "for index, layer in enumerate([net.linear1, net.linear2, net.linear3]):\n",
        "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
        "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
        "    print(\n",
        "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
        "    )\n",
        "# The non-LoRA parameters count must match the original network\n",
        "assert total_parameters_non_lora == total_parameters_original\n",
        "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
        "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
        "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
        "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
        "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eYpOHDRZ-zl",
        "outputId": "f1ed2702-7b4f-4c36-f5c9-0a1065150fa3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
            "Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n",
            "Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n",
            "Total number of parameters (original): 2,807,010\n",
            "Total number of parameters (original + LoRA): 2,813,804\n",
            "Parameters introduced by LoRA: 6,794\n",
            "Parameters incremment: 0.242%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze the non-Lora parameters\n",
        "for name, param in net.named_parameters():\n",
        "    if 'lora' not in name:\n",
        "        print(f'Freezing non-LoRA parameter {name}')\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Load the MNIST dataset again, by keeping only the digit 9\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "exclude_indices = mnist_trainset.targets == 9\n",
        "mnist_trainset.data = mnist_trainset.data[exclude_indices]\n",
        "mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n",
        "# Create a dataloader for the training\n",
        "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
        "\n",
        "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
        "train(train_loader, net, epochs=1, total_iterations_limit=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-u5bXXMaCAn",
        "outputId": "90d6149c-6292-4c79-d0c4-2f99a3991b53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Freezing non-LoRA parameter linear1.bias\n",
            "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear2.bias\n",
            "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
            "Freezing non-LoRA parameter linear3.bias\n",
            "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:  99%|█████████▉| 99/100 [00:03<00:00, 31.77it/s, loss=0.115]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the frozen parameters are still unchanged by the finetuning\n",
        "assert torch.all(net.linear1.parametrizations.weight.original == original_weights['linear1.weight'])\n",
        "assert torch.all(net.linear2.parametrizations.weight.original == original_weights['linear2.weight'])\n",
        "assert torch.all(net.linear3.parametrizations.weight.original == original_weights['linear3.weight'])\n",
        "\n",
        "enable_disable_lora(enabled=True)\n",
        "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
        "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
        "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
        "assert torch.equal(net.linear1.weight, net.linear1.parametrizations.weight.original + (net.linear1.parametrizations.weight[0].lora_B @ net.linear1.parametrizations.weight[0].lora_A) * net.linear1.parametrizations.weight[0].scale)\n",
        "\n",
        "enable_disable_lora(enabled=False)\n",
        "# If we disable LoRA, the linear1.weight is the original one\n",
        "assert torch.equal(net.linear1.weight, original_weights['linear1.weight'])"
      ],
      "metadata": {
        "id": "ClG8YQ1EaFVF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with LoRA enabled\n",
        "enable_disable_lora(enabled=True)\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUv0J8jmaFYy",
        "outputId": "a2e2c57f-6e1f-4733-bf81-042a1255c712"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:16<00:00, 62.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.876\n",
            "wrong counts for the digit 0: 130\n",
            "wrong counts for the digit 1: 28\n",
            "wrong counts for the digit 2: 122\n",
            "wrong counts for the digit 3: 262\n",
            "wrong counts for the digit 4: 179\n",
            "wrong counts for the digit 5: 59\n",
            "wrong counts for the digit 6: 40\n",
            "wrong counts for the digit 7: 214\n",
            "wrong counts for the digit 8: 200\n",
            "wrong counts for the digit 9: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with LoRA disabled\n",
        "enable_disable_lora(enabled=False)\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx9hjHmbaFcj",
        "outputId": "bcf1f420-0d4e-4395-c448-228b06bbd03d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 1000/1000 [00:08<00:00, 116.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.954\n",
            "wrong counts for the digit 0: 31\n",
            "wrong counts for the digit 1: 17\n",
            "wrong counts for the digit 2: 46\n",
            "wrong counts for the digit 3: 74\n",
            "wrong counts for the digit 4: 29\n",
            "wrong counts for the digit 5: 7\n",
            "wrong counts for the digit 6: 36\n",
            "wrong counts for the digit 7: 80\n",
            "wrong counts for the digit 8: 25\n",
            "wrong counts for the digit 9: 116\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpVk1eTeeOSV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}